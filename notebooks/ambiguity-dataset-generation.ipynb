{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vhodlevskyi/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3508: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Access the OpenAI key\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = openai_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"\"\"You are tasked with generating multiple-choice questions for a medical institution exam. In this case, the correct answer should be highly ambiguous, meaning it should be very difficult to distinguish from the incorrect answers.\n",
    "\n",
    "Your task is to create one question with four answer choices:\n",
    "\n",
    "One correct answer (factually accurate)\n",
    "Three incorrect answers that are logically connected to the correct answer but introduce subtle misconceptions.\n",
    "The incorrect answers should be plausible distractors, meaning a student with basic medical knowledge might mistakenly select them.\n",
    "The correct answer must be randomly assigned to one of A, B, C, or D.\n",
    "\n",
    "Example:\n",
    "Question: What is a pneumothorax?\n",
    "A. Gas effusion in the pleural cavity (Correct Option)\n",
    "B. Fluid buildup in the pleural cavity (Incorrect but plausible – confused with pleural effusion)\n",
    "C. A collapsed alveolus (Incorrect but plausible – confused with atelectasis)\n",
    "D. An inflammation of the pleura (Incorrect but plausible – confused with pleuritis)\n",
    "\n",
    "Now generate a question following these guidelines.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_prompt = \"\"\"You are tasked with generating multiple-choice questions for a medical institution exam. In this case, the correct answer should be obviously different from the incorrect answers, leading to minimal or no ambiguity.\n",
    "\n",
    "Your task is to create one question with four answer choices:\n",
    "\n",
    "One correct answer (factually accurate)\n",
    "Three incorrect answers that are unrelated but still within the medical domain (e.g., different medical fields or completely different concepts).\n",
    "The incorrect answers must not be plausible distractors for someone with basic medical knowledge.\n",
    "The correct answer must be randomly assigned to one of A, B, C, or D.\n",
    "\n",
    "Example:\n",
    "Question: What is the primary function of hemoglobin?\n",
    "A. Transporting oxygen in the blood (Correct Option)\n",
    "B. Digesting carbohydrates (Clearly unrelated – digestive system)\n",
    "C. Conducting nerve impulses (Clearly unrelated – nervous system)\n",
    "D. Filtering toxins from the blood (Clearly unrelated – renal system)\n",
    "Correct option: A\n",
    "\n",
    "Now generate a question following these guidelines.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\", temperature = 0.4, api_key = openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCQQuestion(BaseModel):\n",
    "    question: str = Field(description=\"The multiple-choice question\")\n",
    "    option_a: str = Field(description=\"The first answer option labeled 'A'\")\n",
    "    option_b: str = Field(description=\"The second answer option labeled 'B'\")\n",
    "    option_c: str = Field(description=\"The third answer option labeled 'C'\")\n",
    "    option_d: str = Field(description=\"The fourth answer option labeled 'D'\")\n",
    "    correct_option: str = Field(description=\"This consists only a letter of correct option\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq_parser = JsonOutputParser(pydantic_object=MCQQuestion)\n",
    "\n",
    "positive_prompt_template = PromptTemplate(\n",
    "    template=\"{prompt}.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"prompt\": positive_prompt, \"format_instructions\": mcq_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_chain = positive_prompt_template | model | mcq_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt_template = PromptTemplate(\n",
    "    template=\"{prompt}.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"prompt\": negative_prompt, \"format_instructions\": mcq_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_chain = negative_prompt_template | model | mcq_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lisa = pd.read_csv(\"../data/lisa_sheets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T14:51:42.419908Z",
     "iopub.status.busy": "2024-11-21T14:51:42.419473Z",
     "iopub.status.idle": "2024-11-21T14:51:42.427528Z",
     "shell.execute_reply": "2024-11-21T14:51:42.426197Z",
     "shell.execute_reply.started": "2024-11-21T14:51:42.419872Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/train_test_split/train_folders.json\", \"r\") as train_file:\n",
    "    train_folders = json.load(train_file)\n",
    "\n",
    "# Reading the test folders\n",
    "with open(\"../data/train_test_split/test_folders.json\", \"r\") as test_file:\n",
    "    test_folders = json.load(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_folder(folder):\n",
    "    if folder in train_folders:\n",
    "        return 'train'\n",
    "    elif folder in test_folders:\n",
    "        return 'test'\n",
    "    else:\n",
    "        return 'unknown' \n",
    "\n",
    "df_lisa['dataset_split'] = df_lisa['folder'].apply(classify_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset_split\n",
       "train    3169\n",
       "test     1524\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lisa.dataset_split.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Openai api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import concurrent.futures\n",
    "\n",
    "def generate_question_parallel_gpt(content, chain):\n",
    "    \"\"\"Function to process a single content item and generate a question.\"\"\"\n",
    "    try:\n",
    "        generated_question = chain.invoke({\"query\": content})\n",
    "        return generated_question\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for content: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_in_parallel_df(df, chain, max_workers=10):\n",
    "    \"\"\"Run the question generation in parallel for a DataFrame.\"\"\"\n",
    "    questions = [None] * len(df)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_index = {\n",
    "            executor.submit(generate_question_parallel_gpt, content, chain): index \n",
    "            for index, content in enumerate(df['content_gpt'])\n",
    "        }\n",
    "\n",
    "        processed_count = 0 \n",
    "        for future in concurrent.futures.as_completed(future_to_index):\n",
    "            index = future_to_index[future]  \n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    questions[index] = result  \n",
    "            except Exception as e:\n",
    "                print(f\"Unhandled exception in processing item at index {index}: {e}\")\n",
    "\n",
    "            # Update and print progress every 100 items\n",
    "            processed_count += 1\n",
    "            if processed_count % 100 == 0:\n",
    "                print(f\"{processed_count} samples processed...\")\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_lisa['positive_generated_question'] = run_in_parallel_df(df_lisa, positive_chain)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 176 ms, sys: 0 ns, total: 176 ms\n",
      "Wall time: 3.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = run_in_parallel_df(df_lisa.iloc[:10], negative_chain)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_lisa['negative_generated_question'] = run_in_parallel_df(df_lisa, negative_chain)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_options_preserve_order(question_dict):\n",
    "    \"\"\"Shuffles the answer options while preserving the JSON key order.\"\"\"\n",
    "    options = ['option_a', 'option_b', 'option_c', 'option_d']\n",
    "    option_values = [question_dict[opt] for opt in options]\n",
    "    \n",
    "    # Identify the correct answer before shuffling\n",
    "    correct_answer = question_dict['correct_option'].lower()\n",
    "    correct_value = question_dict[f'option_{correct_answer}']\n",
    "    \n",
    "    # Shuffle option labels while keeping key order\n",
    "    shuffled_options = options.copy()\n",
    "    random.shuffle(shuffled_options)\n",
    "\n",
    "    # Assign new shuffled values while preserving key order\n",
    "    shuffled_dict = {\n",
    "        'question': question_dict['question'],\n",
    "        'option_a': None,\n",
    "        'option_b': None,\n",
    "        'option_c': None,\n",
    "        'option_d': None,\n",
    "        'correct_option': None\n",
    "    }\n",
    "\n",
    "    for new_key, old_value in zip(shuffled_options, option_values):\n",
    "        shuffled_dict[new_key] = old_value\n",
    "\n",
    "    # Find the new correct option key\n",
    "    new_correct_option = next(opt for opt, val in shuffled_dict.items() if val == correct_value)\n",
    "    shuffled_dict['correct_option'] = new_correct_option[-1].upper()  # Convert 'option_x' to 'X'\n",
    "\n",
    "    return shuffled_dict\n",
    "\n",
    "df_lisa['shuffled_positive_generated_question'] = df_lisa['positive_generated_question'].apply(shuffle_options_preserve_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lisa['shuffled_negative_generated_question'] = df_lisa['negative_generated_question'].apply(shuffle_options_preserve_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lisa = df_lisa.drop(columns=['positive_generated_question', 'negative_generated_question'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lisa = df_lisa.rename(columns={\n",
    "    'shuffled_negative_generated_question': 'negative_generated_question',\n",
    "    'shuffled_positive_generated_question': 'positive_generated_question'\n",
    "}, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the first dataset with normalized positive_generated_question\n",
    "df_positive = df_lisa[['folder', 'id', 'content_gpt', 'dataset_split', 'positive_generated_question']].copy()\n",
    "\n",
    "df_positive = df_positive.join(df_positive['positive_generated_question'].apply(pd.Series))\n",
    "df_positive = df_positive.drop(columns=['positive_generated_question'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correct_option\n",
       "D    1214\n",
       "C    1177\n",
       "A    1167\n",
       "B    1135\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_positive.correct_option.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the second dataset with normalized negative_generated_question\n",
    "df_negative = df_lisa[['folder', 'id', 'content_gpt', 'dataset_split', 'negative_generated_question']].copy()\n",
    "\n",
    "# Expand the dictionary in negative_generated_question into separate columns\n",
    "df_negative = df_negative.join(df_negative['negative_generated_question'].apply(pd.Series))\n",
    "df_negative = df_negative.drop(columns=['negative_generated_question'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive.to_csv(\"../data/data_for_finetuning/ambiguity/positive_prompt_ambiguity.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative.to_csv(\"../data/data_for_finetuning/ambiguity/negative_prompt_ambiguity.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5935182,
     "sourceId": 9704705,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6010915,
     "sourceId": 9806364,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6015287,
     "sourceId": 9812103,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6137604,
     "sourceId": 9975354,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
