import json
import ollama
import argparse
import sys
import time

from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

parser = argparse.ArgumentParser()
parser.add_argument('-l', "--llm", type=str, default='Mistral')
parser.add_argument('-t', "--temperature", type=float, default=0.5)

args = parser.parse_args()

input_file = 'mcqs_answerability.json'
output_file = 'answerability'
model = None

#CHANGE THE NAME OF THE MODELS 
if args.llm == 'Mistral':
    output_file += f'_mistral_16_{args.temperature}.json'
    model = 'mistral-small:24b-instruct-2501-fp16'
elif args.llm == 'Instruct':
    output_file += f'_mistral_8_{args.temperature}.json'
    model = 'mistral-small:24b-instruct-2501-q8_0'
elif args.llm == 'Llama8':
    model = 'llama3.1:8b-instruct-q8_0'
    output_file += f'_Llama_8_{args.temperature}.json'
elif args.llm == 'Llama16':
    model = 'llama3.1:8b-instruct-fp16'
    output_file += f'_Llama_16_{args.temperature}.json'
elif args.llm == 'Llama3.3_70b':
    model = 'meta-llama/Meta-Llama-3-70B-Instruct'
    output_file += f'_Llama_3.3_70b_{args.temperature}.json'
else:
    sys.exit('ERROR Dataset !!!')


with open(input_file, 'r') as file:
    mcqs = json.load(file)

results = []

#Template
template = "Answer the following {subject} multiple-choice question:\n\nQuestion: {question}\nOptions:\n{options}\n\n"
template += "Provide only the letter corresponding to the correct answer (A, B, C, or D) without additional information in this format:'Answer':'<Letter of the answer>'"

#Prompt
prompt = ChatPromptTemplate.from_template(template)

#Model and chain
model = OllamaLLM(model=model, temperature=args.temperature, format='json', num_predict=4096)

excepts = []

for idx, mcq in enumerate(mcqs):
    question = mcq['question']
    options = f"A: {mcq['opa']}\nB: {mcq['opb']}\nC: {mcq['opc']}\nD: {mcq['opd']}"
    correct_answer = mcq['cop']

    chain = prompt | model

    beg = time.time()

    llm_response = chain.invoke({
        'subject': mcq['subject_name'].lower(),
        'question': question,
        'options': options
    })

    end = time.time() - beg

    try:
        llm_response = eval(llm_response)
        if 'Answer' in llm_response:
            llm_answer = llm_response['Answer']
        elif 'answer' in llm_response:
            llm_answer = llm_response['answer']
        else:
            llm_answer = llm_response
    except:
        llm_answer = llm_response
        excepts.append(idx)   

    is_correct = llm_answer == correct_answer.upper()

    results.append({
        'id': mcq['id'],
        'correct_option': correct_answer,
        'llm_response': llm_answer,
        'is_correct': is_correct,
        'time': end
    })


with open(output_file, 'w') as file:
    json.dump(results, file, indent=4)

with open('excepts_'+output_file, 'w') as file:
    json.dump(excepts, file, indent=4)
    
